\chapter{Iterative development}\label{chapter:prototype}

% Shirley et al. : human-centered design process
%This iterative process can be human-centered design process described in \cite{shirley:2009}.

This chapter describes how the idea presented in chapter \ref{chapter:whitebox} is tested and improved through different development cycles.


\section{Methodology}\label{chapter:prototype:section:methodology}




\section{Iteration 1: paper prototype}\label{chapter:prototype:section:paper}

The research questions can be devided into two broad categories: questions related to insight gaining, i.e., is the visualization understandable, and questions related to usability, i.e., is the application 'user friendly' - note the quotation marks, as Nielsen points out that usability is ambiguously defined by the term user friendliness\cite{nielsen:1993:UE:529793}.

The user study was carried out on five participants. This number is sufficient, as most of the errors discovered were rather similar, and it is likely that further user tests would point out the same flaws. Nielsen also describes reasons why five test users is often sufficient in [2].

The questions related to insight gaining try to follow the insight gaining process as described by Yi et al. in [3]. This is a four step process consisting out of the followin steps: 'provide overview', 'adjust', 'detect pattern' and 'match mental model'.

\subsection{Results}

\subsection{Insight gaining}

The first step aimed at forming a first mental model of the system without interacting with the visualization. When participants were asked to describe what they saw and try to explain the visualization, all of the participants identified the edges as certain relationship between artists. Most of them interpreted the relationship encoded in the edges as a content-based relationship. For example music that was similar in genre are connected.

Blue edges were usually correctly associated with the highlighted active user profile. This insight made one of the users see that the edges represent a co-occurance relationship, i.e., if a user has any set of two items in his/her profile, these items are connected.

In some cases it had to be emphasized that certain items came from the user’s own library and others were not, in other words these are the suggestions. If the user made the connection with the learned fact that blue nodes and edges corresponded to items that were already owned, item suggestions were easy to point out. In some cases this waited until the second step of the evalution process.

In the second step, interaction with the paper prototype was allowed. The users were asked what they expected the possible interaction methods would entail. All of them listed (left) mouse clicks. Also dragging and scrolling were suggested by some participants, but these weren’t included in the design. Some users tried to click on edges as well as the nodes. Of course, the user was told that clicking an edge would have no effect – only clicking nodes and user icons.

Most participants started by clicking another user’s icon and noted the added highlights in red in the graph. For one user the tasks in this step were a mere confirmation of the already established mental model. For most users this turned out to be an important moment in adjusting the first model. When alternating between clicking users icons, as well as between artist nodes and artist nodes and user icons the other users were able to correct their model in this step to finally form the correct picture of what the visualization was trying to convey. For two users this took significantly more time than for the other two remaining users.

The realization of what the relationship encoded in the edges, means, is key to grasping the whole idea behind the visualization. Once this was understood, all users could point out an item recommendation that was more favourable than another suggestion. For example using the total number of links to the active user profile, the total amount of related users, or a strong connection with a particular favourite item.

In conclusion, one user managed to get the complete mental model correct in the first step of the insight gaining process. The others were able to correct it in the second step. The model helped identifying a particular suggestion as more reliable than an other one.

\subsection{Usability}

On the usability side the main functionality, namely adding an item suggestion to the active user profile, turned out to be quite easy. Once the user had seen the effect of clicking a node, the ‘add item to library’ was quickly found.

Subjective data was through a SUS questionnaire. The resulting scores averaged 77 out of a maximum of 100; the lowest score being 65 and the highest score 85.


\subsection{Conclusion}

In general it took the test users quite some to understand the visualization completely. Of course textual information was kept at a bare minimum to see how much could be derived from the visualization without extra aid, and also the interaction with a paper prototype is not as smooth as with a real application.

In the end the results were satisfactory as all the users got a grasp of how the application works and could see how it can help them identify good recommendations.

The results from the SUS questionnaire also indicate that the application is not extremely complicated and useful to some extent.


\section{Iteration 2: digital prototype}\label{chapter:prototype:section:digital}

\subsection{Research questions}

The focus of the study was on how the conversion from paper to digital was successful. Apart from this the study also tried to answer some other research questions. In general, the main questions were the following:

Was the conversion from paper to digital prototype successful? Were there improvements or did the conversion introduce new issues?
What is the quality of the help files? Were there parts that needed editing? Were there parts that should be included that were not, and vice versa?
Is there certain functionality users would like to see added?
Method

The method used was very similar to the methods used when testing of the paper prototype. Again, the user was asked to study the application, first without and later with interaction. Between each step the user was asked to explain what he/she thought the application did and what its use could be. After this the user had to provide a reasoning for choosing a particular recommendation.

Next the user read and reviewed the help file.

Finally the user was asked to give feedback on the application and give suggestions for improvements or new functionality.

At the end of the test, the test users also completed a SUS questionnaire.

\subsection{Results}

7 users participated in the user tests. Here are some of the remarks by users:

There should be some way to find out which lines correspond to which user when clicking an item on the graph;
The help files should contain a table of contents;
Some images or clips would be nice in the help files as well;
Deselecting an item or user should be triggered by clicking somewhere outside the graph as well. Perhaps a “clear selection” button would do as well.
Additional option to alter the number of items shown.
Filter for showing items.
Different colours for hovering and selecting, although others thought this wasn’t really necessary. Perhaps add an option to choose between different encodings.
Legend for different colours.
The average SUS score was 73 with the lowest starting from 65 to the highest score of 90. The results were spread quite evenly between these extremes.




\section{Iteration 3: application implementation}\label{chapter:prototype:section:implementation}









