\chapter{Literature study}\label{chapter:literature_study}

The term 'human-computer interaction' describes a phenomenon where two actors, the human or \emph{user} and the computer or \emph{system}, share a communication channel. The communication channel is a representation of data of the system towards the user. It is clear that both actors will impose certain restrictions on this visual communication channel\cite{shirley:2009, ware:2004}. In this chapter we will try to establish who these actors are, what this communication channel looks like, and what kind of restrictions are imposed upon the visual communication channel. 



% --------------------------- The User ----------------------------
% 
% -----------------------------------------------------------------
\section{The user}\label{chapter:literature_study:section:user}

Who is the first actor? The answer to this question is of course very broad, so first we will try to list what we actually want to know. The goal is to allow the user to gain insight into the system through an interactive, visual explanation system. In conclusion, two specific questions arise:

\begin{itemize}
	\item how does a human gain insight?
	\item what kind of limitations are imposed by the user on the design of an interactive visualization?
\end{itemize}

The following subsections try to establish an answer to these questions. Most of the ideas in these subsections are drawn from a papers by Yi. et al. \cite{yi:2008}, North et al. \cite{north:2006}, Klein et al. \cite{Klein:2006:MSS:1158821.1159015, klein:2006:MSS:1175882.1176017}, and a book by Colin Ware \cite{ware:2004}.


\subsection{Insight gaining}\label{chapter:literature_study:section:user:subsection:insight}

What is insight\index{insight}? In \cite{north:2006} it is argued that insight is not a well-defined term. A formal definition might be too restrictive to capture its essence, and yet too broad to be useful. To quantify insight, \cite{north:2006} and \cite{yi:2008} list characteristics that allow a finer evaluation:

\begin{itemize}
	\item \textbf{Complex}: insight is complex in the sense that involves large amounts of data that form cognitive constructs, rather than individual units;
	\item \textbf{Deep}: insight is self-generating in a way, as insight provides a starting point for insight on the next level;
	\item \textbf{Qualitative}: insight is subjective, uncertain and can have multiple levels of resolution;
	\item \textbf{Unexpected}: insight is usually unpredictable, serendipitous and creative;
	\item \textbf{Relevant}: insight is deeply embedded in the data domain: it gives data meaning as it connects data to the existing domain knowledge;
\end{itemize}

The quality of insight can then be determined by quantifying each of these characteristics\cite{north:2006}. The previously described collection of properties defines insight. Now we will look at the closely related concept of sensemaking. The next paragraphs describe how a user can arrive at insight.


\subsubsection{Sensemaking}\label{chapter:literature_study:section:user:subsection:insight:subsubsection:sensemaking}

\emph{Sensemaking}\index{sensemaking} plays an important part in insight gaining \cite{yi:2008}. The definitions for sensemaking may vary. We adapt the definition presented by \cite{Klein:2006:MSS:1158821.1159015} and \cite{yi:2008}.

In \cite{Klein:2006:MSS:1158821.1159015} sensemaking is looked at from a psychological perspective, a perspective of human-centered computing, and the perspective of naturalistic decision making. Sensemaking is then defined as follows: "sensemaking is a motivated, continuous effort to understand connections in order to anticipate their trajectories and act effectively"\cite{Klein:2006:MSS:1158821.1159015}.

Based on the discussion in \cite{klein:2006:MSS:1175882.1176017} and \cite{yi:2008}, Soo Yi et al. describe the process of sensemaking. Sensemaking is a:

\begin{itemize}
	\item \textbf{Cyclic and iterative proceduce}: consisting out of a generation loop searching for representations, a data coverage loop instantiating the representations and finally shift representations;
	\item \textbf{Creation procedure}: being more about reasoning than discovery;
	\item \textbf{Retrospective procedure}: as people construct a framework and assign relevant information to a place withing this framework. If the data fits the framework well, the framework is confirmed, otherwise it may be updated or discarded;
\end{itemize}

An important remark made in \cite{Klein:2006:MSS:1158821.1159015} is that data fusion algorithms can reduce information overload, but they also pose challenges to sensemaking if the human can't form an accurate mental model of the machine, to understand why and how the algorithms are doing what they are doing.


\subsubsection{Processes of insight gaining}\label{chapter:literature_study:section:user:subsection:insight:subsubsection:processes}

Although sensemaking can play an important part in gaining insight, it is not the only path to arrive at insight \cite{yi:2008}. Soo Yi et al. \cite{yi:2008} identify four processes through which insight is established. Note that these processes are intertwined and often used together to generate insights. The processes are as follows\cite{yi:2008}:

\begin{itemize}
	\item \textbf{Provide overview}: in this process the individual gains understanding of the big picture of a dataset of interest. It allows the user to make a distinction between what is known to him/her and what is not;
	\item \textbf{Adjust}: in this process a person will explore a dataset by adjusting the level of abstraction and/or the range of selection. Typical actions involve filtering and grouping of data;
	\item \textbf{Detect pattern}: in this process the user will try to identify specific distributions, trends, frequencies, outliers or structure in the dataset;
	\item \textbf{Match mental model}: in this process the gap between data and cognitive model is bridged, reducing cognitive load and linking the present visual information with real-world knowledge.
\end{itemize}

The link with sensemaking is found in the cyclic and iterative nature of sensemaking - provide overview, adjust and detected pattern can be applied iteratively, as well as its creative and retrospective aspects - adjust and detect pattern create hypotheses and test them through various interaction techniques\cite{yi:2008}.


\subsubsection{Improving insight}\label{chapter:literature_study:section:user:subsection:insight:subsubsection:improving}

Yi et al. \cite{yi:2008} identify several ways in which the insight gaining process can be made more efficient. They list the system's interactivity, the quality of visual encodings and usability among others, as possible enablers for increased insight gaining. Naturally, improvident designs will act as barriers rather than enablers in the insight gaining process.

Interactivity of the system promotes the user's engagement into the dataset. Spending more time with the data will allow users to form more detailed and accurate hypotheses, and as a result greater insight\cite{yi:2008}. At the same time, while using the visualization, the user will become moe skilled at a task over time. Nonetheless, bare in mind that when performing long and tedious search tasks, vigilance will become an important aspect as well in the efficiency of data exploration\cite{ware:2004}.

Similarly visual encodings that are counter-intuitive will also increase the cognitive load. Other barriers on insight gaining are clutter, occlusion and data overload\cite{yi:2008}.

Usability is another aspect that may have an impact on the insight gaining process, as controls that are hard to use will inevitably occupy some of the cognitive capacity of the user\cite{yi:2008}. In the ISO standard ISO 9241-11, usability is defined as "the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use"\cite{usabilitynet:2006:standards}.

Note that usability\index{usability} should not be considered a one-dimensional property of a user interface. Nielsen identifies several characteristics of usability in applications\cite{nielsen:1993:UE:529793}:

\begin{itemize}
	\item \textbf{Learnability}: if the system is easy to learn, the user can get started quickly;
	\item \textbf{Efficiency}: if the system is efficient to use, it will be possible to complete more work in less time;
	\item \textbf{Error rate and severity}: if the system should be robust and minimize faults;
	\item \textbf{Memorability}: once the system is learned, acquired skills should not be forgotten easily;
	\item \textbf{Satisfaction}: the system should be pleasant to use.
\end{itemize}


\subsection{Interactive visualization}\label{chapter:literature_study:section:user:subsection:interactive}

As we now have a better understanding of what insight is, we will try to establish how the insight gaining process works through visual data mining\index{visual data mining} and interactive visualization\index{visualization!interactive visualization}. The relation between insight gaining and data visualization has been pointed out in other research. Colin Ware \cite{ware:2004} describes interactive visualization as an "internal interface between the user and the computer in a problem solving system". Keim \cite{keim:2002} notes that "idea behind visual data exploration, is to present data in a visual form, allowing the user to gain insight into the data, draw conclusions, and directly interact with the data".

In a chapter on visualization\index{visualization} in \cite{shirley:2009}, Tamara Munzner describes visualization as follows: "visualization allows the user to offload cognition to the perceptual system, using graphical data representations as a form of external memory. Therefore, by augmenting human capabilities, the data analyst is aided to understand, explore and form hypotheses of the data"\cite{shirley:2009}. In conclusion, the visual data exploration\index{visual data exploration|see{visual data mining}} process can then be understood as a hypothesis generation process\cite{keim:2002}.

A reoccurring theme in visualization design is "overview first, zoom and filter, and details on demand"\cite{keim:2002, shirley:2009, ware:2004}. First, the user looks for patterns of interest in the data space. Next the user focuses on one or more of these patterns, and starts looking at the data on a more detailed level. The user can then draw his/her conclusions and explore the data space further\cite{keim:2002}. 

In what follows, we try to describe how a human interacts with interactive visualization on a cognitive level. It will be clear that some parallels can be drawn with the insight gaining process. This should come as no surprise, since these processes are intertwined\cite{keim:2002, ware:2004, yi:2008}.

In \cite{ware:2004}, interactive visualization is characterized by three classes of feedback loops:

\begin{itemize}
	\item \textbf{Data selection and manipulation loop}: the user selects and moves objects that are selected through simple interactions based on eye-hand coordination;
	\item \textbf{Exploration and manipulation loop}: the user tries to find his/her way through a large visual data space;
	\item \textbf{Problem solving loop}: the user forms hypotheses about the data and refines them through an augmented visualization process.
\end{itemize}

The next subsections describe each feedback loop in greater detail.


\subsubsection{Data selection and manipulation loop}\label{chapter:literature_study:section:user:subsection:interactive:subsubsection:loops1}

The quality of performance of selecting and manipulating data on a screen, depends on certain factors. Colin Ware discusses the following attributes:

\begin{itemize}
	\item \textbf{Reaction time}: This is the amount of time for a user to identify and select certain objects\cite{ware:2004}.
	\item \textbf{Types of interaction}: different types of interaction will have a different influence on user performance.
	\item \textbf{Learning}: The speed at which a user performs a task may decrease over time, as the user becomes more skilled at executing the task.
\end{itemize}

Each of these factors has been evaluated and since, they are captured in various laws. We will discuss some of them, as listed by Ware et al. in \cite{ware:2004}.

The reaction time is given by the \emph{Hick-Hyman law}\index{Hick-Hyman law}: $Time_{reaction} = a + b \log_{2}(C)$ with $C$ the number of choices and $a$ and $b$ empirically determined constants. This formula has been derived from experiments in which subjects had to press one of two buttons depending on the color of a light that was turned on or turned off\cite{ware:2004}.

The reaction-time may be influenced by many other factors such as the amount of visual noise, the distinctness of the signal and so on. If a person is allowed to make mistakes, the subject will respond faster, but at a cost of loss of accuracy\cite{ware:2004}. When performing long and tedious search tasks, vigilance will become an important aspect as well\cite{ware:2004}.

In \cite{ware:2004} different kinds of interactions are discussed such as selection, hover querries and path tracing. The time required for selecting an object in a two-dimensional space is determined by \emph{Fitt's law}\index{Fitt's law}: $Time_{selection} = a + b \log_{2}(D/W+1.0)$ with $D$ the distance to the target, $W$ the width of the target and $a$ and $b$ empirically determined constants\cite{ware:2004}.

A common way of selecting objects is through hover queries: the user drags a cursor over an object\cite{ware:2004}.

Another type of interaction is path tracing. The speed at which a user can trace or follow a given path is given by $v = W/\tau$ with velocity $v$, $W$ the path width and $\tau$ a constant depending on the motor control system of the user\cite{ware:2004}.

When a task is repeated over time, the user will become more efficient at performing this task. Depending on the difficulty of the task, this learning effect will be more or less prominent. The \emph{power law of practice}\index{power law of practice} describes this speed up \cite{ware:2004}: $\log(T_{n})=C-\alpha\log(n)$ in which $T_{n}$ is the time required to perform the task at the $n$-th trial, with $C=log(T_{1})$ and $\alpha$ the steepness of the learning curve.

Some actions are easier to learn than others. This is related to the \emph{stimulus-response (S-R) compatibility}\index{stimulus-response compatibility}. The stimulus-response compatibility refers to the way in which skills that have been learned before through everyday experience, are applied to computer control design. However it is not necessary to produce a whole virtual reality to create excellent computer interfaces. Research shows that mismatches between the interface and real world experience are not necessarily detrimental to the efficiency of human-computer interaction. Ware concludes that "it would be naive to conclude that computer interfaces should evolve toward VR simulations of real-world tasks (...). The magic of computers is that a single button click can often accomplish as much as a prolonged series of actions in the real world"\cite{ware:2004}.


\subsubsection{Exploration and manipulation loop}\label{chapter:literature_study:section:user:subsection:interactive:subsubsection:loops2}

In the second loop the user navigates through the data space. The basic navigation control loop is described as an iterative process that involves two distinct aspects\cite{ware:2004}:

\begin{itemize}
	\item \textbf{Human}: the user gains understanding of the data space through a logical, spatial model. Parts of this model may be encoded in the longterm memory, on the condition that the data space is maintained for a long enough period of time;
	\item \textbf{Computer}: the visualization may be updated and refined from data mapped onto the spatial model.
\end{itemize}

When exploring spatial maps, a user is confronted with the \emph{focus-context problem}\index{focus-context problem}, i.e., "the problem of finding detail in a larger context"\cite{ware:2004}. The objective is to see the relation between the larger context and the details, rather than finding details. Ware goes on to discern between spatial, structural and temporal scales in which the focus-context problem manifests itself\cite{ware:2004}.

Ware and Mitchell remark that the human visual system is already well-adapted to the spatial focus-context problem\cite{ware:2004}. Therefore, they argue that when designing a display, it should already try to take a maximal advantage of these perceptual skills.

The time it takes to navigate between two points in the data space to gather information is called the \emph{cost of knowledge}\index{cost of knowledge}\cite{ware:2004}. To keep the cost of knowledge low, various interaction techniques have been developed, such as distortion, rapid zooming, elision, and multiple windows\cite{ware:2004}. In section \ref{chapter:literature_study:section:interaction} we will take a closer look at some of these techniques.


\subsubsection{Problem solving loop}\label{chapter:literature_study:section:user:subsection:interactive:subsubsection:loops3}

The problem solving loop can be described through means of a \emph{visual thinking algorithm}\index{visual thinking algorithm}\cite{ware:2004}. Such an algorithm combines perceptual and cognitive actions into a process, as the user interacts with the visualization and explores the data space. As we want to keep the \emph{cost of knowledge}\index{cost of knowledge} low, it is obvious that the cost and time complexity of each of these actions should be kept at a minimum. The cognitive system that runs these algorithms, is made up out of several different components. Ware lists the following components\cite{ware:2004}:

\begin{itemize}
	\item \textbf{Early visual processing}: Early visual processing occurs at the lowest level, where elementary features such as color, texture information, local edges, and motion are extracted from the visual image.
	\item \textbf{Pattern perception}: Pattern perception occurs by combining the extracted features, patterns can be detected within the visual image.
	\item \textbf{Eye movements}: A map schedules tasks to explore proto-patterns through eye-movements. These proto-patterns correspond to the patterns that have a high probability to be relevant to the current task.
	\item \textbf{The intrasaccadic scanning loop}: Information is processed serially when focusing on an area of interest.
	\item \textbf{Working memory}: The working memory forms an intermediary between the incoming patterns and long-term memory. The currency of information in the working memory are \emph{object files}\index{object file}. Object files are a combination of visual attributes and semantic meaning.
	\item \textbf{Mental imagery}: The ability to visualize internal images, whivh can be combined with external images, such as data visualization, to construct and test hypotheses about the visualized data.
	\item \textbf{Epistemic actions}: Epistemic actions are actions that support information search, such as eye movement, and mouse selections. Out of all epistemic actions, eye movements have the lowest cost.
	\item \textbf{Visual queries}: A visual query\index{visual query} translates a hypothesis into a cognitive task. The result of this query can be a pattern or lack of a pattern. To reduce errors and increase efficiency, patterns are typically small, as the capacity of the visual working memory\index{visual working memory} is rather limited.
	\item \textbf{Computational data mappings}: The computer maps data onto the screens. Sometimes the data is transformed before doing so. An important consideration is that the interval between the human action and the result on the screen should be low, promoting the so-called \emph{priciple of transparency}\index{priciple of transparency}. The objective of transparency is that the user will have the illusion of directly manipulating the data on the screen.
\end{itemize}

In \cite{ware:2004}, Ware and Mitchell list ten different visual thinking algorithms. We will describe our own visual thinking algorithm in chapter \ref{chapter:whitebox}, based on these algorithms.


% different kinds of operations (page 398)


% toward visual thinking algorithms
% ---------------------------------

% visual working memory + attention
% ---------------------------------
% visual working memory capacity
%			simple shapes
%			implications?
%					glyphs,
%					change blindness,
%					gist
%			egocentric coordinate map (spatial information)
%	attention
% object files, coherence fields, gist


% long-term memory
% ----------------
% contents
% capacity
% structure
% low redundancy


% Creative thinking
% -----------------
%		building new things from old things



% -------------------------- The System ---------------------------
% 
% -----------------------------------------------------------------
\section{The system}\label{chapter:literature_study:section:computer}

Before describing the visual communication channel between human and computer, it is necessary to know what exactly has to be explained through visualization. Therefore we will take a closer look at the system, i.e., the recommender system.

A recommender system\index{recommender system} is a system that computes item suggestions for users based on a ratings of related items in the user's profile and/or history. Additional information can be incorporated into the recommendation algorithm\index{recommendation algorithm} to refine suggestions. Several categorizations of these techniques are proposed in literature \cite{bostandjiev:2012, burke:2002, herlocker:2000, melville:2002:CCF:777092.777124, celma:2008:phd}.

One of the incentives behind creating recommender system is the 'long-tail phenomenon'\index{Long Tail}. This phenomenon can be explained as follows. Physical retail and warehouses can only keep a subset of all the available items in stock. These items are usually the most popular items on the market. Online vendors however, such as \emph{Amazon}\footnote{\url{http://www.amazon.com/}}, can offer a vastly larger subset of these items to clients, including also less popular and/or less known items\cite{rajaraman:2012}. Typically the long-tail phenomenon is visualized in a graph in which items are ordered by their popularity on the horizontal axis against the popularity rating on the vertical axis, as can be seen on figure \ref{figure:longtail}. Physical stores will offer only items in the first part of the graph, whereas the online vendors will also sell items from the remaining 'long tail' of the graph\cite{rajaraman:2012, celma:2008:phd}. Recommender systems then provide a means to find relevant items within this much larger range of items\cite{rajaraman:2012}. They enable to "connect supply and demand, introducing consumers to these new and newly available goods and driving demand down the tail "\cite{anderson:2006:LTW:1197299, celma:2008:phd}.

\begin{figure}%
	\begin{center}
		\includegraphics[width=250px]{img/longtail}%
	\end{center}
	\caption{The long-tail: Items ordered by popularity are layed out against their popularity rating. Most of the items reside in the long tail of the graph. Companies such as Amazon can offer a vastly greater subset of the total item space.}%
	\label{figure:longtail}%
\end{figure}

The success of recommender systems to achieve this objective has been the subject of some research, e.g. \cite{levy:2010} and \cite{celma:2008:phd}. Different classes of recommender algorithms favour different properties\cite{burke:2002, shani:2011:9780387858197}. This is elaborated further in subsection \ref{chapter:literature_study:section:computer:subsection:challenges}. For now its enough to see that the quality of recommendations is not uniquely defined. For example, in order to increase coverage of the item space, achieving high serendipity and novelty may be desired more than high recommendation accuracy, depending on the application context\cite{shani:2011:9780387858197, tripathi:2011, celma:2008:phd}.

Typical applications of recommender systems are product recommenders for online retailers, movie and music recommenders such as \emph{Netflix}\footnote{\url{http://www.netflix.com/}} and \emph{Last.fm}\footnote{\url{http://www.last.fm/}}, and news article recommenders in online news services\cite{levy:2010, rajaraman:2012, celma:2008:phd}.

Recommender system have opened up new possibilities in the landscape of online retail, and as a result, spurred the interest of businesses in this field. A remarkable initiative was the Netflix challenge\index{Netflix challenge}. In 2006, Netflix Inc. offered a prize to beat the performance of their recommendation algorithm by 10 percent. It gave a significant boost to the research on recommendation algorithms, and yielded a winning algorithm in September 2009\cite{bell:2007, rajaraman:2012}.


\subsection{Properties of recommendation algorithms}\label{chapter:literature_study:section:computer:subsection:properties}

in \cite{herlocker:2004:ECF:963770.963772} and \cite{shani:2011:9780387858197}, Herlocker et al. and Shani et al.  respectively, compare the performance of recommender systems. They list a number of metrics for recommendation algorithms. The resulting set of properties consist out of the following characteristics:

\begin{itemize}
	\item \textbf{Accuracy}: The accuracy of item recommendations. There are three broad classes of prediction accuracy measures:
	\begin{itemize}
		\item the prediction of the rating given by a user;
		\item the prediction whether or not a user will actually use the item (for example adding to a queue) opposed to predicting the rating itself;
		\item the prediction of a ranking among items rather than an explicit rating of each item independently.
	\end{itemize}
	\item \textbf{User preference}: The opinion of certain users may be more valuable than the opinion of others.
	\item \textbf{Coverage}: The proportion of items that the recommender system can recommend is referred to as catalog coverage. Another measure in this respect is the percentage of all items that are recommended to users. Finally we can also look at the diversity of the recommended items. Coverage can also mean the proportion of users or user interactions for which the system can recommend items.
The cold start problem relates to coverage as it measures the coverage for a specific type of users, namely new users.
	\item \textbf{Confidence}: Confidence in the recommendation can be defined as the systems trust in its recommendations or predictions. The most common measurement of confidence is the probability that the predicted value is indeed true, or the interval around the predicted value where predefined portion of the true values lie. Confidence bounds can be used to filter recommended items where the confidence in the predicted value is below some threshold.
	\item \textbf{Trust}: Trust refers to the user's trust in the system, as opposed to confidence.
	\item \textbf{Novelty}: Novel recommendations are recommendations for items that the user did not know about.
	\item \textbf{Serendipity}: Serendipity is a measure of how surprising the successful recommendations are. One can think of serendipity as the amount of relevant information that is new to the user in a recommendation, or alternatively as deviation from the 'natural' prediction.
	\item \textbf{Diversity}: Diversity is generally defined as the opposite of similarity. Note that an increase in diversity may correlate to a decrease in accuracy.
\end{itemize}


\subsection{A classification of recommendation algorithms}\label{chapter:literature_study:section:computer:subsection:algorithms}

Based on classifications presented in \cite{burke:2002} and \cite{celma:2008:phd}, a categorization of different types of recommendation strategies can be identified. We will only discuss the two most prominent ones, namely collaborative filtering (CF)\index{recommendation algorithm!collaborative filtering} and content-based filtering (CB)\index{recommendation algorithm!content-based filtering}\cite{herlocker:2000, rajaraman:2012}, and list some hybrid strategies. In the literature on recommender systems other general approaches that are commonly identified, are utility-based filtering, knowledge-based filtering, demographic filtering, and expert-based filtering\cite{burke:2002, bostandjiev:2012}.


\subsubsection{Collaborative recommendation}\label{chapter:literature_study:section:computer:subsection:algorithms:subsubsection:cf}

\emph{Collaborative recommendation}\index{collaborative recommendation|see{collaboratibe filtering}} aggregates item ratings by users. By establishing overlaps between ratings in the corresponding user profiles, the system generates new item recommendations\cite{burke:2002, herlocker:2000}. A typical user profile in a collaborative system consists of a vector of items and their ratings, that continuously augmented as the user interacts with the system over time\cite{burke:2002}. 

For CF-based recommendation\index{CF|see{collaborative filtering}}, there are two classes of entities: users $U$ and items $I$. The data itself can then be represented by a utility matrix\index{utility matrix} $A$. The entries $a_{i,j}$ of the utility matrix represent what is known about the degree of preference of user $u_{i}$ and item $i_{j}$\cite{rajaraman:2012}. As can be seen in figure \ref{figure:utilitymatrix}, the utility matrix will have many blanks as well. The goal of the recommendation algorithm is then to fill in the blanks\cite{rajaraman:2012}.

\begin{figure}%
\begin{center}
	\includegraphics[width=250px]{img/utility_matrix}%
\end{center}
	\caption{The utility matrix $A$.}%
	\label{figure:utilitymatrix}%
\end{figure}

% Similarity / distance functions ...
In order to calculate the blanks, there is a variety of similarity functions that has been developed over the years. Often some sort of distance function is used to compute distance between profile vectors. If the distance is small, profiles will most likely have a high similarity\cite{rajaraman:2012}. The discussion of the mathematics behind each of the algorithms is beyond the scope of this thesis.

% clustering, pearson correlation, ...


\subsubsection{Content-based recommendation}\label{chapter:literature_study:section:computer:subsection:algorithms:subsubsection:cbf}

\emph{Content-based recommendation}\index{content-based recommendation|see{content-based filtering}} learns a profile of the user's interests based on the features present in objects the user has rated. New recommendations can then be generated based on a similarity function on these features\cite{burke:2002, pazzani:2007:CRS:1768197.1768209}.

% Feature vectors ...
When applying content-based filtering, the choice of similarity or classification function will have a significant impact on the quality of the recommendations. More importantly though, is the choice of features. To ensure good performance, these features should also be extracted easily from large quantities of data.

% examples of how feature vectors can be constructed

Depending on the type of item that is being recommended, different approaches can be applied to extract features and construct \emph{feature vectors}\index{feature vector}. Textual information is often extracted using some kind of stemming system\cite{rajaraman:2012}. \emph{Stemming}\index{stemming} is a technique to use root forms that capture a common meaning of certain words, rather than words themselves. For example 'compute', 'computation', 'computer', and 'computes' all have a reference to an underlying concept. Words are given a quantification of relevance to this root form, the so-called TF.IDF (term frequency times inverse document frequency) score\index{TF.IDF}\index{term frequency times inverse document frequency|see{TD.IDF}}. This score is computed in terms of the frequency of the word in a document, number of documents in which it occurs, and the total number of documents. The resulting structure consists out of tuples of root forms and quantification values\cite{pazzani:2007:CRS:1768197.1768209, rajaraman:2012}. The words with the highest scores are the words that characterize the document\cite{rajaraman:2012}. A downside of stemming is that the process may cause the loss of contextual information for each word\cite{pazzani:2007:CRS:1768197.1768209}.

In \cite{bostandjiev:2012} and \cite{melville:2002:CCF:777092.777124} web crawlers are used to gather and extract features from online documents. In \cite{melville:2002:CCF:777092.777124} the web crawler collects properties such as movie title, director, cast, genre, plot summary, plot keywords user comments, reviews and so on, of each movie in the data set. Each property or feature is a 'bag of words'\index{bag of words} that is used in a naive Bayesian text classifier\index{Bayesian classifier}. This way each item can be categorized and the profile can be 'learned'\cite{melville:2002:CCF:777092.777124}.

Tags are very useful as well. Although they can be generated from text, for complex objects such as images and music, tag generation relies on user input\cite{rajaraman:2012}. Nonetheless, emerging technologies such as the 'search by image' option introduced by \emph{Google}\footnote{\url{http://www.google.be/imghp?hl=nl&tab=Ti}}, allow to retrieve web sites, documents and key words related to the given image\cite{google:2011:afbeeldingen}.

Mathematical models of music and images also allow for feature extraction. For example, in a study by Johnson et al. \cite{johnson:2008}, \emph{wavelets}\index{wavelet} were used to construct feature vectors from high resolution paintings. These were in turn used to classify paintings based on several clasification algorithms. A training set of Van Gogh paintings was used with the objective to classify other paitings as either genuine Van Goghs or forgeries. The algorithms were able to actually classify paintings with a high accuracy\cite{johnson:2008}.

Similarly algorithms have been developed to classify music based on content features. There are various types of acoustic features that can be extracted. In \cite{Li:2006:TIM:2219090.2219562} a distinction is made between rhythmic content features, pitch content features and timbral content features.

The research performed by Tzanetakis et al. in \cite{tzanetakis:2001} and \cite{tzanetakis:2002} is based on a the MARSYAS system. This system is an application that uses various techniques, such as Mel-Frequency Cepstral Coefficients (MFCC) and Short-term Fourier Transform Features (FFT) to retrieve acoustic features from audio signals \cite{Tzanetakis:1999:MFA:972850.972857}. In addition to these features, also wavelet-based feature extraction is used to retrieve rhythmic features from audio content \cite{tzanetakis:2001, tzanetakis:2002}. Using this system, music files could be classified into different genres based on rhythmic features\cite{tzanetakis:2001}.

A similar research is conducted by Li et al. in \cite{Li:2006:TIM:2219090.2219562}. In addition to previously mentioned methods, also lyric-based classification is used. Apart from music classification, also music similarity search is performed. In this case, the feature vectors from the classification using MARSYAS features and wavelet-histograms can be used to form a hierarchy in which similar audio signals can be detected \cite{Li:2006:TIM:2219090.2219562}.

% CLassification methods?

%These methods illustrate how mathematical models can be applied for feature extraction, vector generation and classification.


\subsubsection{Hybrid recommendation}\label{chapter:literature_study:section:computer:subsection:algorithms:subsubsection:hf}

\emph{Hybrid filtering}\index{hybrid filtering} combines two or more recommendation algorithms\cite{burke:2002}. In \cite{burke:2002} a number of hybrid recommendation strategies are discussed. Burke et al. list seven different approaches for combining recommendation algorithms:

\begin{itemize}
	\item \textbf{Weighed}: The scores (or votes) of several recommendation techniques are combined together to produce a single recommendation.
	\item \textbf{Switching}: The system switches between recommendation techniques depending on the current situation.
	\item \textbf{Mixed}: Recommendations from several different recommenders are presented at the same time.
	\item \textbf{Feature combination}: Features from different recommendation data sources are thrown together into a single recommendation algorithm.
	\item \textbf{Cascade}: One recommender refines the recommendations given by another.
	\item \textbf{Feature augmentation}: Output from one technique is used as an input feature to another.
	\item \textbf{Meta-level}: The model learned by one recommender is used as input to another.
\end{itemize}

Each of these combinations also has its advantages and disadvantages. Not necessarily all combinations will be successful, and not all of them have been implemented \cite{burke:2002}.



\subsection{Challenges for recommender systems}\label{chapter:literature_study:section:computer:subsection:challenges}

% Cold start / ramp up problem, first rater, new user, new item, gray sheep, profile trust (bad users), sparsity, black box

Each recommendation technique has benefits as well as drawbacks. Some of these apply to all or most types of recommendation strategies, while others are only relevant to certain cases.

Both CF and CB-based recommendation algorithms suffer from the ramp-up\index{ramp-up|see{cold start}} problem in one way or the other. The 'ramp-up' or 'cold start' problem\index{cold start} (although they may refer to slightly different problems depending on the literature) is dual problem that encompasses two distinct, yet related problems as defined in \cite{burke:2002}:

\begin{itemize}
	\item \textbf{New User}\index{cold start!new user}: when a recommender system uses ratings by its users to compute item recommendations, it is hard to find neighbours for a user, who has a limited profile. As user profiles tend to build up over time, new users usually fall in this category.
	\item \textbf{New Item}\index{cold start!new item}: a new item will most likely not have that many ratings associated with it, and as a result will not be easily recommended. This 'new item problem' typically emerges when new items are constantly added to the system; for example when browsing a constant stream of news articles. When new articles are introduced, not many users have had the chance yet to rate these items. In the case of a news feed, an additional problem is that these items are short-lived, meaning that at some point these item profiles will most likely stop receiving any ratings at all.
\end{itemize}

Both of these issues translate themselves into a sparse regions in the utility matrix. It is worth noting that content-based recommendation algorithms suffer less from the \emph{new item} problem, as these tend to rely on features that are inherent to the items themselves, rather than user generated content. This is one of the reasons hybrid approaches can provide a solution to collaborative filtering\cite{burke:2002}. For example, in \cite{melville:2002:CCF:777092.777124}, content-based predictors are used to create pseudo-user ratings to reduce sparsity of the utility matrix, used in a collaborative algorithm.

%Another solution is to use clustering

A problem that is typical of collaborative filtering is the 'gray sheep problem'\index{gray sheep}\cite{burke:2002, herlocker:2000}. The gray sheep problem occurs when a user falls between different clusters of users that may have contradicting item ratings. As a result, it is hard to determine how to classify the user\cite{burke:2002}.

% UV decomposition

Another issue with recommendation systems is that these system often appear as 'black boxes' towards the end user\index{black box}. The complexity of the algorithms used, prevents the user from understanding the recommendation rationale\cite{zhao:2010}. This problem decreases the acceptance by the user of item suggestions. One of the solutions for this problem, proposed by Herlocker et al. in \cite{herlocker:2000}, is to provide an explanation system, i.e., the white box\index{white box}, on top of the recommender system that explains the recommendation process. This can be done through providing a transcript of the system's reasoning or through visualizations\cite{herlocker:2000}.


% more specifics on recommender properties




% ------------------------ The interface --------------------------
% 
% -----------------------------------------------------------------
\section{The interface}\label{chapter:literature_study:section:interaction}

% What will we study?
% Link with previous sections?
In this section we will take a closer look at the visual communication channel.

Shirley et al.\cite{shirley:2009} lists three distinctive limitations:

\begin{itemize}
	\item \textbf{Computational capacity}: time complexity and memory usage of algorithms must allow a responsive user interface, especially in the case of interactive visualization. This is also a requirement to meet the \emph{principle of transparency}\index{principle of transparency} discussed in section \ref{chapter:literature_study:section:user:subsection:interactive:subsubsection:loops3}.
	\item \textbf{Display capacity}: there is a trade-off between the benefits of maximizing the \emph{information density}\index{information density}, i.e., the measure of the amount of encoded information against the amount of unused space, and causing visual overload.
	\item \textbf{Human perceptual and cognitive capacity}: optimizing the cognitive cost is one of the key aspects that make up a successful visualization, as visual and non-visual memory capacity are limited\cite{ware:2004}, as we saw earlier in section \ref{chapter:literature_study:section:user:subsection:interactive}.
\end{itemize}

There has been done extensive research in this domain. Over the last two decades various new information visualization techniques have been developed. These techniques can be classified based on three criteria \cite{keim:2002}:

\begin{itemize}
	\item \textbf{Data}: a classification based on the structure and type of data;
	\item \textbf{Technique}: a classification based on characteristics of visualization techniques;
	\item \textbf{Interaction and distortion}: a classification based on the way in which interaction between user and visualization is enabled.
\end{itemize}


\subsection{Types of data}\label{chapter:literature_study:section:interaction:subsection:datatypes}

% context
% wat can be visualized, data types (recommender data)

Information visualization\index{information visualization}\index{infovis|see{information visualization}} has been focusing on on data sets that lack inherent spatial semantics, thus posing a challenge to map the abstract data onto a two-dimensional screen space\cite{keim:2002}.

There are different types of data and their characteristics will have an influence on the type of visualization. Tables of data consist out of rows, representing items, and columns, representing the data dimensions, or 'attributes'. The number of dimensions is referred to as the dimensionality\index{dimensionality} of the data set\cite{keim:2002}. There are three different kinds of dimensions, namely\cite{shirley:2009}:

\begin{itemize}
	\item \textbf{Quantitative}: numerical data on which arithmetic can be applied;
	\item \textbf{Ordered}: an enumeration that has a definite order;
	\item \textbf{Categorical}: data that has no specific ordering, and is distinguished by name only.
\end{itemize}

Relational data\index{relational data} on the other hand consists out of nodes\index{graph!node} and links or 'edges'\index{graph!edge}\cite{keim:2002, shirley:2009}. Both nodes and edges can have associated attributes.

In \cite{keim:2002} also text and hypertext, and algorithms and software are discussed as examples of other types of data. In the case of text and hypertext, standard visualizations are hard to use, as they cannot be described easily in terms of numbers. As a result, the data is first transformed into description vectors. Next, these vectors can be used in a visualization. Examples of software and algorithm visualization are flow diagrams, presentation using a graph-based structure of source code, and so on\cite{keim:2002}.


\subsection{Visual encoding and visual channels}\label{chapter:literature_study:section:interaction:subsection:encoding}

%Visual encoding principles

\emph{Visual encoding}\index{visual encoding} is defined as the mapping of data set attributes to a visual representation. The choice of visual encoding is one of the central problems in the visualization design\cite{shirley:2009}.

Visual encoding takes place through visual channels. A visual encoding corresponds to a graphical element, or ‘mark’. Examples of visual channels are spatial position, color, size, et cetera. The dimension of the mark may vary: a point is a zero-dimensional mark, a line a one-dimensional one, an area a two-dimensional one and so on.

A visual encoding has the following characteristics, as described in\cite{shirley:2009}:

\begin{itemize}
	\item \textbf{Distinguishability}: the ability of a user to distinguish between visual encodings;
	\item \textbf{Seperability}: Separable visual channels are opposed to integral visual channels, which are focused together on a pre-conscious level. Separable visual channels are safe to use for encoding multiple dimensions;
	\item \textbf{Pop-out}: selecting a channel and make it visually stand out from all the others.
\end{itemize}

%Visual channels

There is a variety of possible visual channels that a visualization designer can turn to in order to create a visual encoding, such as color, spatial position, size, shape, orientation, and so on. The performance of the visual encoding (through a visual channel) depends on the type of data, i.e. quantitative, ordered or categorical \cite{shirley:2009}. Figure \ref{figure:encodings} gives an overview of the performance for each category, adapted from \cite{shirley:2009}. Note that spatial position is the most accurate for each data type.

%Figure 1: Visual encoding performance for each data type
\begin{figure}%
	\begin{center}
		\includegraphics[width=250px]{img/visualencodings}%
	\end{center}
	\caption{Visual encoding performance for each data type, ordered from best to worst.}%
	\label{figure:encodings}%
\end{figure}


%\subsubsection{Colour}\label{chapter:literature_study:section:interaction:subsection:encoding:subsubsection:colour}

In \cite{shirley:2009} colour is considered in terms of three separate channels: \emph{hue}, \emph{saturation} and \emph{brightness}. This allows for different encodings. Just like for most visual channels, the choice of the channel (hue, saturation or brightness) depends heavily on the type of data:

\begin{itemize}
	\item \textbf{Quantitative data}: uses a color map, a range of color values that can be continuous or discrete. It is recommended to use lightness instead of hue, as lightness has an implicit perceptual ordering. Moreover the human eye responds most to strong luminance. Hue on the other hand has a small range (around twelve values that can be reliably distinguished, including background and neutral colors);
	\item \textbf{Ordered data}: lightness and saturation are advised. As mentioned before, these have an implicit perceptual ordering;
	\item \textbf{Categorical data}: hue can be successfully applied for categorical data, keeping in mind its small range.
An important remark is that roughly 10\% of men is red-green color deficient. If a coding uses red and green, it may be wise to apply redundant coding using lightness or saturation in addition to hue \cite{shirley:2009}.
\end{itemize}

Spatial layouts form other visual channels. Although these tend to be the most accurate, spatial layouts in two and three dimensions have several weaknesses \cite{shirley:2009}:

\begin{itemize}
	\item \textbf{Occlusion}: parts of the data set become hidden by others. In the case of the mapping of abstract dimensions onto spatial positions, understanding the details of a three-dimensional visualization may be challenging, even if the user is allowed to change viewpoints;
	\item \textbf{Perspective distortion}: again, in the case of the mapping of abstract dimensions onto spatial positions, distances may convey meaning that may be distorted through perspective.
	\item \textbf{Text in arbitrary orientations}: special care has to be taken with text, as it may become very hard to read depending on the orientation.
\end{itemize}


\subsection{Techniques}\label{chapter:literature_study:section:interaction:subsection:techniques}

% Techniques to overcome limitations of visual channels (see also ware)
Visualization techniques try to deal with the limitations listed in the introduction of this section. Tamara Munzner \cite{shirley:2009} acknowledges that there is a trade-off to be made concerning the interaction cost: on the positive side, interaction allows for data exploration in a larger information space than could be understood in a single static image. The downside is that interaction costs, other than the obvious implementation and computational costs, include the requirements for human time and attention\cite{shirley:2009}. Since attention drops significantly after the first hour\cite{ware:2004}, it might become a critical factor in the quality of the visualization. This view is also supported by Ware's findings described in section \cite{chapter:literature_study:section:user:subsection:interactive:subsubsection:loops3}, as eye-movements have a much lower cost than other epistemic actions\cite{ware:2004, ware:2008}.

Based on the discussion by Ware and Mitchell in \cite{ware:2004}, Keim in \cite{keim:2002}, and Munzner in \cite{shirley:2009}, we will discuss a number of visualization, interaction, and data reduction techniques.

\subsubsection{Visualization techniques}\label{chapter:literature_study:section:interaction:subsection:techniques:subsubsection:infovis}

% 3D --> cf. ware: in order to maximize the portion of the data space being visualized...


\subsubsection{Interaction techniques}\label{chapter:literature_study:section:interaction:subsection:techniques:subsubsection:interaction}

Several techniques can be applied to solve the focus-context problem\index{focus-context problem} described in section \ref{chapter:literature_study:section:user:subsection:interactive:subsubsection:loops2}, further, such as distortion and rapid zooming\cite{ware:2004}. Keim \cite{keim:2002} also lists a number of techniques for interactive visualization. A summary based on \cite{ware:2004}, \cite{shirley:2009} and \cite{keim:2002} is presented here:

\begin{itemize}
	\item \textbf{Dynamic projections}: Data projections are dynamically changed, allowing the user to explore a multidimensional data set.
	\item \textbf{Interactive filtering}: The data set is interactively partitioned into different segments in order to focus on different subsets through querying, browsing or more advanced methods.
	\item \textbf{Interactive zooming}: The data representation is interactively changed, moving between different resolutions. A distinction can be made between two kinds of zooming\cite{shirley:2009, ware:2004}:
	\begin{itemize}
		\item \textbf{Geometric/rapid zooming}: Rapidly changing viewpoint in a two- or three-dimensional space, similar to adjusting the ratio of the focal length on binoculars;
		\item \textbf{Semantic zooming/elision}: Changing the representation of an abstract object visualization, for example expanding and collapsing parts of a large structure that was represented by a single visual entity.
	\end{itemize}
	\item \textbf{Interactive distortion}: Parts of the data are displayed with a high level of detail (focus), while others are shown with a lower level of detail (context). Examples are 'fish-eye' distortion, bifocal displays and hyperbox.
\end{itemize}


\subsubsection{Data reduction techniques}\label{chapter:literature_study:section:interaction:subsection:techniques:subsubsection:reduction}

% transformation of data, reducing data, clustering, dimensionality reduction, ...



\subsection{Graph-based visualization}\label{chapter:literature_study:section:interaction:subsection:graphs}

% graph-based visualization






% ------------------------ Related work- --------------------------
% 
% -----------------------------------------------------------------
\section{Related work}\label{chapter:survey:section:applications}

In this section we will take a look at visual explanation systems that already exist and that have been evaliated as well. Five different applications are discussed: PeerChooser, Pharos, SFVis, SmallWorlds, and TasteWeights.

Next a comparison is made between these systems, based on a number of objectives for explanation systems listed by Tintarev and Masthoff in \cite{tintarev:2007:SER:1547550.1547664}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% APPLICATIONS
\subsection{PeerChooser}\label{chapter:survey:section:applications:subsection:peerchooser}

\emph{PeerChooser}\index{PeerChooser} is a "collaborative movie recommender system with an interactive graphical explanation interface"\cite{odonovan:2008}. It aims to address the black box problem, described in section \ref{chapter:literature_study:section:computer:subsection:challenges}\cite{odonovan:2008}.

The application shows a peer graph of the user's neighbourhood. The visualization uses a force-directed graph layout where the on-screen distance between nodes corresponds an approximation of the node similarity. The active user is able to manipulate this neighbourhood by repositioning nodes of the graph. To deal with the high dimensionality of the data, extra cluster nodes are added to the vsiualization. These cluster peer nodes by genre\cite{odonovan:2008}.

By moving a cluster node closer towards the active user's node, all user nodes associated with this cluster will be drawn closer towards the active user node. As a result, these profiles will be temorary considered more similar than before. Similarly individual users can be moved closer towards or further away from the active user node\cite{odonovan:2008}.

Figure \ref{figure:peerchooser} shows the graph-based interface of the PeerChooser application.

\begin{figure}%
	\begin{center}
		\includegraphics[width=300px]{img/peerchooser}%
	\end{center}
	\caption{The PeerChooser interface.}%
	\label{figure:peerchooser}%
\end{figure}



\subsection{Pharos}\label{chapter:survey:section:applications:subsection:pharos}

Content-centric social websites, such as blogs and discussion forums, contain vast amounts of fast growing information. Recommendation systems have been developed to help users find the information they are looking for. The \emph{Pharos}\index{Pharos} application tries to address two distinctive problems that present themselves in this context: the cold start problem and the black box problem\cite{zhao:2010}, as described earlier in section \ref{chapter:literature_study:section:computer:subsection:challenges}.

As they hope to overcome previously defined problems, Zhao et al. \cite{zhao:2010} collect and visualize content-related social behaviour. The resulting data set is tranformed into a social map. The social map provides a context for new users, addressing the cold start problem. Secondly, the user can explore the social map to increase understanding and user interaction, in an effort to overcome the black box problem.

The generation of the social map takes place through the following three step process:

\begin{itemize}
	\item \textbf{Community extraction}: a map depicting 'which users are talking about what'. Starting from either relationships, people or content communities can be derived;
	\item \textbf{Community/item/people ranking}: the next step is to rank these communities. The 'hotness' can be measured on content, people authorities and so on;
	\item \textbf{Community labeling}: describing what each community is about.
\end{itemize}

An example of what the resulting visualization looks like, is depicted in figure \ref{figure:pharos}.

\begin{figure}%
	\begin{center}
		\includegraphics[width=300px]{img/pharos}%
	\end{center}
	\caption{The Pharos social map. Colours indicate activity within a certain group.}%
	\label{figure:pharos}%
\end{figure}




\subsection{SFVis}\label{chapter:survey:section:applications:subsection:sfvis}

SFVis (Social Friends Visualization) is an application that helps users explore and find friends interactively under a context of interest. The system is a hybrid approach of social tags and social networks. The SFVis framework transforms a data model (social tags and social networks) into a visual form. Users can both manipulate the input and visuals on demand.

Social tags can form a network. Within this structure clusters may arise. From this cluster tag network a hierarchy is derived. A compound graph is generated from the tag hierarchy and social networks.

A mapping function assigns actors in a social network to a tag tree. The actor similarity algorithm in SFVis considers both structure similarity in a social network and semantic similarity in a tag network. These scores will allow the recommendation system to compute friend suggestions.

SFVis uses circular visualizations for the different trees and graphs for both views as well as interaction with the user.

\begin{figure}%
	\begin{center}
		\includegraphics[width=300px]{img/sfviz_tagtree}%
	\end{center}
	\caption{SFViz graphical user interface: tag tree.}%
	\label{figure:sfviz}%
\end{figure}



\subsection{Smallworlds}\label{chapter:survey:section:applications:subsection:smallwords}

In \cite{gretarsson:2010}, Gretarsson et al. used the Facebook API to create an application to generate social recommendations for Facebook users. Unfortunately the Facebook API does not support unauthorized reading of item preference information beyond the immediate friend group. This would not have been a problem unless traditional collaborative filtering strategies tend to produce item suggestions of inferior quality for small items. In this case however the research team relies on the social filtering through the active user's peer group\cite{gretarsson:2010}.

\emph{SmallWorlds}\index{SmallWorlds} is "a visual interactive graph-based interface that allows users to specify, refine and build item-preference profiles"\cite{gretarsson:2010}. The system promotes transparancy in the recommendation process, and gives the user a sense of control over the recommendation process through interactions. This way, Gretarsson et al. try to further overcome the limitations of their recommender system\cite{gretarsson:2010}.

SmallWorlds uses a five-layered design to create suggestions:

\begin{enumerate}
	\item the active user's node;
	\item the active user's profile items;
	\item friends who have items in common with the active user;
	\item items that are not in the active user’s profile, but are liked by friends in layer $3$;
	\item friends who have no items in common with the active user and items in their profiles, but not items in the profiles of friends in layer $3$.
\end{enumerate}

The user can move nodes in each layer further or closer towards the active user's node to adjust the weights of each node. This is used in combination with similarity functions to calculate the suggestions. Figure \ref{figure:smallworlds} shows a screenshot of the application.

\begin{figure}%
	\begin{center}
		\includegraphics[width=300px]{img/smallworlds}%
	\end{center}
	\caption{The SmallWords interface.}%
	\label{figure:smallworlds}%
\end{figure}


\subsection{TasteWeights}\label{chapter:survey:section:applications:subsection:tasteweights}

TasteWeights is a hybrid recommender with an interactive graphical user interface\cite{bostandjiev:2012}. The application allows the user to express his/her preferences changing the weights of incoming data sources.

One of the challenges Bostandjiev et al. try to address is that "social web APIs and other data sources are constantly evolving, and traditional recommender system techniques such as automated collaborative filtering need to adapt to the changing environment of the social web"\cite{bostandjiev:2012} (cf. Smallworlds). They introduce two enhancements for the traditional techniques. Multiple web sources, namely from \emph{Facebook}\footnote{\url{https://www.facebook.com/}}, \emph{Twitter}\footnote{\url{https://twitter.com/}} and \emph{Wikipedia}\footnote{\url{http://www.wikipedia.org/}} are combined when computing the recommendation. This combination provides a hybrid of different recommendation strategies, namely: collaborative filtering, expert-based and content-based respectively. The second enhancement is a new user interface that provides transparency into the recommendation process.

There are three levels to be distinguished that are represented visually as well:

\begin{itemize}
	\item \textbf{Profile layer}: liked items on \emph{Facebook};
	\item \textbf{Context layer}: items coming from different sources, namely \emph{Twitter}, \emph{Facebook}, and \emph{Wikipedia};
	\item \textbf{Recommendation layer} containing the actual recommendations.
\end{itemize}

Figure \ref{figure:tasteweights} shows the corresponding visual representation of each of these levels. Edges connect relevant parts between each of these levels on the visualization, in an attempt to explain the provenance of item recommendations. The user can influence the outcome displayed in the recommendation layer by attributing weights to the nodes in the profile layer and context layer\cite{bostandjiev:2012}.

\begin{figure}%
	\begin{center}
		\includegraphics[width=300px]{img/tasteweights}%
	\end{center}
	\caption{The TasteWeights interface.}%
	\label{figure:tasteweights}%
\end{figure}

% research questions: accuracy, user experience, benefit of explanations, hybrid better than CF?







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% COMPARATIVE STUDY
%% -----------------
%%
%% Based on :
%%		* properties
%%		* visualization techniques
%%		* tintarev criteria
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparative study of visual explanation systems for item recommendation}

The following listing is adapted from \cite{tintarev:2007:SER:1547550.1547664}:

\begin{itemize}
	\item \textbf{Transparency}: explain how the system works;
	\item \textbf{Scrutability}: allow users to tell;
	\item \textbf{Trust}: increase users' confidence in the system;
	\item \textbf{Effectiveness}: help users make good decisions;
	\item \textbf{Persuasiveness}: convince users to try or buy;
	\item \textbf{Efficiency}: help users make decisions faster;
	\item \textbf{Satisfaction}: increase the ease of usability or enjoyment.
\end{itemize}

%Table1 : overview recommender system case studies
In table \ref{table:comparison:properties} an overview is presented of some the different recommender systems and some of their properties. So far most of the applications discussed here used collaborative filtering and some kind of visualization and allowed the user to interact with the system.

Table \ref{table:comparison:techniques} gives an overview of each of the techniques that were used to design and implement the visualizations.

Table \ref{table:comparison:criteria} gives an overview of the performance of each visual explanation system based on the criteria listed by Tintarev and Masthoff in \cite{tintarev:2007:SER:1547550.1547664}.

\begin{center}
	\begin{table}%
		\begin{tabular}{lcr}
			
		\end{tabular}
		\caption{A comparison of the recommender systems, based on recommender system properties.}
		\label{table:comparison:properties}
	\end{table}
\end{center}



\begin{center}
	\begin{table}%
		\begin{tabular}{lcr}
			
		\end{tabular}
		\caption{A comparison of the visual explanation systems, based on visualization, interaction, and data reduction techniques.}
		\label{table:comparison:techniques}
	\end{table}
\end{center}


\begin{center}
	\begin{table}%
		\begin{tabular}{lcr}
			
		\end{tabular}
		\caption{A comparison of the visual explanation systems, based on the criteria by Tintarev and Masthoff listed in \cite{tintarev:2007:SER:1547550.1547664}.}
		\label{table:comparison:criteria}
	\end{table}
\end{center}



